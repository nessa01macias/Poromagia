{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Exacting text from a card "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something extremetly important to take into consideration is the fact that as we will use regions of interest for image extractions, the picture of the cards need to be taken in **exactly** the same angle and the same coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2 is the module import name for opencv-python needed for the cv algorithm\n",
    "import cv2\n",
    "# pillow is needed to editing images, printing them, rotating them...\n",
    "from PIL import Image\n",
    "# exact text from images using pytesseract\n",
    "import pytesseract \n",
    "# basic path works for all the files\n",
    "import sys\n",
    "# array handling\n",
    "import numpy as np\n",
    "# for text detecting\n",
    "import easyocr\n",
    "# finding the edges of the images\n",
    "import imutils\n",
    "# looping through images\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Loading an image with opencv. They become a bunch of numbers in a array that refer to [r,g,b] which means \"per pixel\" how much color of each is used. in order to display an array, we use pillow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# parameter 0 makes it black & white, now it only has one element per pixel. pixel is inside the range of 0 (completely black) to 255 (completely white)\n",
    "image = cv2.imread(\"4.jpg\")\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "# i rotate the image to vertical\n",
    "gray_image = cv2.rotate(gray_image,cv2.ROTATE_90_CLOCKWISE)\n",
    "# Image from Pillow makes the pic printable as a image and not only an array\n",
    "Image.fromarray(gray_image).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Binarization. Images will have different shadings and we would like all of them to be as similar to each other as possible, therefore threshold is needed. If we didn't use the contract, probably the text extraction would not be even half of efficient as it can be after applying the threshold. [check out doc!]\n",
    "(https://docs.opencv.org/4.x/d7/d4d/tutorial_py_thresholding.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh, gray_thresh_image = cv2.threshold(gray_image, 120, 240, cv2.THRESH_BINARY)\n",
    "gray_thresh_image = np.asarray(gray_thresh_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Finding edges for localization and pointing out the contours. We need to separate the [card name], [card image] and [card type] from the picture before extracting the text. The previous steps work for the text extraction, as the image extraction needs other kind of preprocessing before getting the image in the region of interest.\n",
    "Something really important to take into consideration is that all the cards must be in the same position, and the same angle everytime the picture is taken to avoid more pre-processing and make the ROI more accurate.\n",
    "https://circuitdigest.com/microcontroller-projects/license-plate-recognition-using-raspberry-pi-and-opencv check out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows  2048  and columns  1536\n"
     ]
    }
   ],
   "source": [
    "num_rows, num_cols = gray_thresh_image.shape\n",
    "print(\"number of rows \",num_rows, \" and columns \",num_cols)\n",
    "# the number of rows and columns are important for the following part-extrations of the images. they will be changing as the proper\n",
    "#values from the arduino come to the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "reader = easyocr.Reader(['en'], gpu = False) # does rasperry pi have gpu? https://www.electromaker.io/blog/article/the-raspberry-pi-now-supports-external-gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Extracting the text from [card type] & [card name]. The regions of interest have been identified and we will try to extract text from them. The best way is to make a function that is called every time we need extraction. \n",
    "https://www.youtube.com/watch?v=owiqdzha_DE&t=384s&ab_channel=DigitalSreeni\n",
    "https://pyimagesearch.com/2020/09/14/getting-started-with-easyocr-for-optical-character-recognition/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need string library to include all ASCII letters\n",
    "import string \n",
    "\n",
    "def clean_text(text:str) -> str:\n",
    "    # ascii_letters include all the letters from english alphabet in lower and upper case\n",
    "    included = string.ascii_letters\n",
    "    # first we treat the text as an array because strings are inmutable in python\n",
    "    new_str = []\n",
    "    for char in text: #h, #o, #l, #a, #!\n",
    "        if char in included or char == ' ' or char == 'â€”':\n",
    "            # appeding to the array if the string is included\n",
    "            new_str.append(char)\n",
    "    # removing extra spaces\n",
    "    new_text = ''.join(new_str)\n",
    "    return new_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(rows, columns) -> str:\n",
    "\n",
    "    # first we implement the region cutting\n",
    "    roi_image = gray_thresh_image[rows[0]:rows[1], columns[0]:columns[1]]\n",
    "    Image.fromarray(roi_image).show()\n",
    "\n",
    "    # proceed to convert it to an array\n",
    "    roi_image = np.array(roi_image)\n",
    "    # reading the text, details = 0 as we only want the text and not the box values, and paragraph in order to avoid a list but get a full word\n",
    "    results = reader.readtext(roi_image, detail=0,paragraph= True)\n",
    "    return clean_text(results[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the text from the top part (card name) is Kenriths Transformation  and from the middle part (card type) is Enchantment Aura\n"
     ]
    }
   ],
   "source": [
    "rows_cardType = [1000, 1200]\n",
    "columns_cardType = [250, 1200]\n",
    "\n",
    "rows_cardName = [250, 500]\n",
    "columns_cardName = [250, 1200]\n",
    "\n",
    "\n",
    "text_cardName = extract_text(rows_cardName, columns_cardName)\n",
    "text_cardType = extract_text(rows_cardType, columns_cardType)\n",
    "print(\"the text from the top part (card name) is\", text_cardName , \" and from the middle part (card type) is\", text_cardType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Glob and trying the process with 15 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path:str) -> int:\n",
    "    image = cv2.imread(path)\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray_image = cv2.rotate(gray_image,cv2.ROTATE_90_CLOCKWISE)\n",
    "    thresh, gray_thresh_image = cv2.threshold(gray_image, 120, 240, cv2.THRESH_BINARY)\n",
    "    gray_thresh_image = np.asarray(gray_thresh_image)\n",
    "    return gray_thresh_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\nessa\\\\Poromagia\\\\Poromagia\\\\back_end\\\\resources\\\\img\\\\1.jpg',\n",
       " 'C:\\\\Users\\\\nessa\\\\Poromagia\\\\Poromagia\\\\back_end\\\\resources\\\\img\\\\10.jpg',\n",
       " 'C:\\\\Users\\\\nessa\\\\Poromagia\\\\Poromagia\\\\back_end\\\\resources\\\\img\\\\11.jpg',\n",
       " 'C:\\\\Users\\\\nessa\\\\Poromagia\\\\Poromagia\\\\back_end\\\\resources\\\\img\\\\12.jpg',\n",
       " 'C:\\\\Users\\\\nessa\\\\Poromagia\\\\Poromagia\\\\back_end\\\\resources\\\\img\\\\13.jpg',\n",
       " 'C:\\\\Users\\\\nessa\\\\Poromagia\\\\Poromagia\\\\back_end\\\\resources\\\\img\\\\14.jpg',\n",
       " 'C:\\\\Users\\\\nessa\\\\Poromagia\\\\Poromagia\\\\back_end\\\\resources\\\\img\\\\15.jpg',\n",
       " 'C:\\\\Users\\\\nessa\\\\Poromagia\\\\Poromagia\\\\back_end\\\\resources\\\\img\\\\2.jpg',\n",
       " 'C:\\\\Users\\\\nessa\\\\Poromagia\\\\Poromagia\\\\back_end\\\\resources\\\\img\\\\3.jpg',\n",
       " 'C:\\\\Users\\\\nessa\\\\Poromagia\\\\Poromagia\\\\back_end\\\\resources\\\\img\\\\4.jpg',\n",
       " 'C:\\\\Users\\\\nessa\\\\Poromagia\\\\Poromagia\\\\back_end\\\\resources\\\\img\\\\5.jpg',\n",
       " 'C:\\\\Users\\\\nessa\\\\Poromagia\\\\Poromagia\\\\back_end\\\\resources\\\\img\\\\6.jpg',\n",
       " 'C:\\\\Users\\\\nessa\\\\Poromagia\\\\Poromagia\\\\back_end\\\\resources\\\\img\\\\7.jpg',\n",
       " 'C:\\\\Users\\\\nessa\\\\Poromagia\\\\Poromagia\\\\back_end\\\\resources\\\\img\\\\8.jpg',\n",
       " 'C:\\\\Users\\\\nessa\\\\Poromagia\\\\Poromagia\\\\back_end\\\\resources\\\\img\\\\9.jpg']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r\"C:\\Users\\nessa\\Poromagia\\Poromagia\\back_end\\resources\\img\\*\"\n",
    "glob.glob(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 15)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new data frame for storing the results\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in glob.glob(path):\n",
    "    print(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "9f0b219ec83649936bce7717ffc92d24e05b7358d200d73bcdb3e34c55ccc427"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
